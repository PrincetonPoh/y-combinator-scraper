{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time, re, os\n",
    "from datetime import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def run_seleniun_and_get_page_source():\n",
    "    url = 'https://www.ycombinator.com/companies'\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.headless = True\n",
    "    try:\n",
    "        path = os.getenv('CHROMEDRIVER_HOME')\n",
    "        driver = webdriver.Chrome(executable_path=path, chrome_options=options)\n",
    "    except Exception as e:\n",
    "        driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    #scroll to the end of the page\n",
    "    check_page_length = 0\n",
    "    try:\n",
    "        while True:\n",
    "            page_len = driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "            time.sleep(0.5)\n",
    "\n",
    "            if check_page_length == page_len:\n",
    "                break\n",
    "            check_page_length = page_len\n",
    "    except:\n",
    "        driver.close()\n",
    "\n",
    "    selenium_web_content = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    get_company_list_block = selenium_web_content.find_all('a', class_='WxyYeI15LZ5U_DOM0z8F no-hovercard')\n",
    "    \n",
    "    return get_company_list_block, driver\n",
    "\n",
    "\n",
    "def get_company_info(soup):\n",
    "    info = {}\n",
    "\n",
    "    company_data = soup.find('div',  class_='my-8 mb-4')\n",
    "    info['link'] = company_data.find('div', class_='flex flex-row items-center px-3 leading-none').a['href']\n",
    "\n",
    "    summary = soup.find('div',  class_='space-y-3')\n",
    "    info['company_name'] = summary.h1.text\n",
    "    info['short_description'] = summary.find('div',  class_='text-xl').text\n",
    "\n",
    "    spans = summary.find_all('span',  class_='ycdc-badge')\n",
    "    info['tags'] = [what.text.replace('Y Combinator Logo', '') for what in spans]\n",
    "    info['description'] = soup.p.text\n",
    "    info['company_socials'] = soup.find('div',  class_='space-x-2')\n",
    "    \n",
    "    spans = []\n",
    "\n",
    "    i = 0\n",
    "    for fact in soup.find('div', class_=\"space-y-0.5\").find_all('span'):\n",
    "        spans.append(fact.text)\n",
    "        try:\n",
    "            key_ = spans[i].lower().replace(' ', '_')[:-1]\n",
    "            info[key_] = spans[i+1]\n",
    "            i += 2\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    info['company_socials'] = [a['href'] for a in soup.find('div',  class_='space-x-2').find_all('a')]\n",
    "\n",
    "    return info\n",
    "\n",
    "\n",
    "def get_founders_info(soup):\n",
    "    founders_info = {}\n",
    "    try:\n",
    "        #? This implementation is for active founders\n",
    "        founders = soup.find('div', class_='space-y-5')\n",
    "        founders_info['active_founders'] = [name.div.text for name in founders.find_all('div', class_='leading-snug')]\n",
    "\n",
    "        all_about = []\n",
    "        for what in founders.find_all('div', class_='leading-snug'):\n",
    "            about_founder = {}\n",
    "            name = what.find('div', class_='font-bold').text\n",
    "            about_founder['name'] = name\n",
    "            about_founder['social_media_links'] = [link['href'] for link in what.find('div', class_='mt-1 space-x-2').find_all('a')]\n",
    "            \n",
    "            all_about.append(about_founder)\n",
    "\n",
    "        founders_info['about_founders'] = all_about\n",
    "        \n",
    "    except:\n",
    "        founders = soup.find('div', class_='space-y-4')\n",
    "        founders_info = {}\n",
    "        all_about = []\n",
    "        for founder in founders.find_all('div', class_='leading-snug'):\n",
    "            about_founder = {}\n",
    "            name = founder.find('div', class_='font-bold').text\n",
    "            founders_info['active_founders'] = [name]\n",
    "\n",
    "            about_founder['name'] = name\n",
    "\n",
    "            divs = [ what for what in founder.find_all('div')]\n",
    "            about_founder['role'] = divs[1].text\n",
    "            about_founder['social_media_links'] = [link['href'] for link in founder.find('div', class_='mt-1 space-x-2').find_all('a')]\n",
    "\n",
    "            all_about.append(about_founder)\n",
    "            \n",
    "        founders_info['about_founders'] = all_about\n",
    "\n",
    "    return founders_info\n",
    "\n",
    "\n",
    "def scrape_info(link_href):\n",
    "    main_url = 'https://www.ycombinator.com'\n",
    "    url = main_url + link_href\n",
    "    source = requests.get(url).text\n",
    "    soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "    company_all_info = get_company_info(soup)\n",
    "    founder_info = get_founders_info(soup)\n",
    "    company_all_info.update(founder_info)\n",
    "\n",
    "    return company_all_info\n",
    "\n",
    "\n",
    "def save_to_csv(scraped_info, savepath):\n",
    "       df = pd.DataFrame(scraped_info)\n",
    "       df = df[['company_name', 'link', 'short_description', 'tags',\n",
    "              'company_socials', 'founded', 'team_size', 'location',\n",
    "              'active_founders', 'about_founders', 'description']]\n",
    "              \n",
    "       df.to_csv(savepath, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all links from Y-combinator\n",
    "`run_seleniun_and_get_page_source`: Used to handle the dynamic scraping of the project. It scrolls the website till it reaches the end of the page. Afterward, beautifulsoup is used to extract all the links to the individual company's page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dd/p_l_68gj2hj_qyc4_9yblp580000gn/T/ipykernel_58152/810837057.py:16: DeprecationWarning: headless property is deprecated, instead use add_argument('--headless') or add_argument('--headless=new')\n",
      "  options.headless = True\n",
      "/var/folders/dd/p_l_68gj2hj_qyc4_9yblp580000gn/T/ipykernel_58152/810837057.py:19: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path=path, chrome_options=options)\n",
      "/var/folders/dd/p_l_68gj2hj_qyc4_9yblp580000gn/T/ipykernel_58152/810837057.py:19: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(executable_path=path, chrome_options=options)\n",
      "/var/folders/dd/p_l_68gj2hj_qyc4_9yblp580000gn/T/ipykernel_58152/810837057.py:21: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selenium closed, handing over to BeautifulSoup\n",
      "Number of companies: 1000\n",
      "Total scroll runtime - 40.743295\n"
     ]
    }
   ],
   "source": [
    "\n",
    "main_url = 'https://www.ycombinator.com'\n",
    "\n",
    "start = dt.now()\n",
    "get_company_list_block, driver = run_seleniun_and_get_page_source()\n",
    "driver.close()\n",
    "\n",
    "\n",
    "print(\"Selenium closed, handing over to BeautifulSoup\")\n",
    "\n",
    "y_company_page_urls = [link['href'] for link in get_company_list_block]\n",
    "length = len(y_company_page_urls)\n",
    "print(f\"Number of companies: {length}\")\n",
    "\n",
    "runtime = (dt.now() - start).total_seconds()\n",
    "print(f'Total scroll runtime - {runtime}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do this for all the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batches = ['W23', 'S22', 'W22', 'S21', 'W21', 'S20', 'W20', 'S19', 'W19', 'S18', 'W18', 'S17', 'W17', 'IK12', 'S16', 'W16', 'S15', 'W15', 'S14', 'W14', 'S13', 'W13', 'S12', 'W12', 'S11', 'W11', 'S10', 'W10', 'S09', 'W09', 'S08', 'W08', 'S07', 'W07', 'S06', 'W06', 'S05']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove known faulty company websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n"
     ]
    }
   ],
   "source": [
    "known_faulty_websites = ['/companies/11874']\n",
    "for known_faulty_website in known_faulty_websites:\n",
    "    try:\n",
    "        y_company_page_urls.remove(known_faulty_website)\n",
    "    except:\n",
    "        # already removed\n",
    "        pass\n",
    "    \n",
    "print(len(y_company_page_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping using multi-threading\n",
    "Multithreading gives a faster and more efficient reuslt, the script ran for about `~62 secs` as compared the the first appoach.\n",
    "\n",
    "`Note:` The runtime can be faster or slower depending on internet connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime - 36.798173\n",
      "[{'name': 'Nathan Blecharczyk', 'social_media_links': ['https://twitter.com/nathanblec', 'https://www.linkedin.com/in/blecharczyk/']}, {'name': 'Brian Chesky', 'social_media_links': ['https://twitter.com/bchesky']}, {'name': 'Joe Gebbia', 'social_media_links': ['https://twitter.com/jgebbia', 'https://www.linkedin.com/in/brianchesky']}]\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures as cf\n",
    "\n",
    "\n",
    "start_thread = dt.now()\n",
    "m_companies = []\n",
    "with cf.ThreadPoolExecutor() as exc:\n",
    "    results = exc.map(scrape_info, y_company_page_urls)\n",
    "\n",
    "    for result in results:\n",
    "        m_companies.append(result)\n",
    "\n",
    "runtime_thread = (dt.now() - start_thread).total_seconds()\n",
    "print(f'Total runtime - {runtime_thread}')\n",
    "print(m_companies[0]['about_founders'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "save_to_csv(scraped_info=m_companies, savepath='ycombinator.csv')\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "y-combinator-scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
