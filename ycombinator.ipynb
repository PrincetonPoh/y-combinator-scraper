{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time, re, os\n",
    "from datetime import datetime as dt\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def run_seleniun_and_get_page_source():\n",
    "    url = 'https://www.ycombinator.com/companies'\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.headless = True\n",
    "    try:\n",
    "        path = os.getenv('CHROMEDRIVER_HOME')\n",
    "        driver = webdriver.Chrome(executable_path=path, chrome_options=options)\n",
    "    except Exception as e:\n",
    "        driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(10)\n",
    "\n",
    "    #scroll to the end of the page\n",
    "    check_page_length = 0\n",
    "    try:\n",
    "        while True:\n",
    "            page_len = driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "            time.sleep(0.5)\n",
    "\n",
    "            if check_page_length == page_len:\n",
    "                break\n",
    "            check_page_length = page_len\n",
    "    except:\n",
    "        driver.close()\n",
    "\n",
    "    selenium_web_content = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    get_company_list_block = selenium_web_content.find_all('a', class_='styles-module__company___1UVnl no-hovercard')\n",
    "    \n",
    "    return get_company_list_block, driver\n",
    "\n",
    "\n",
    "def get_company_info(soup):\n",
    "    info = {}\n",
    "\n",
    "    summary = soup.find('div',  class_='space-y-3')\n",
    "    info['company_name'] = summary.h1.text\n",
    "    info['link'] = soup.find('div', class_='flex flex-row items-center leading-none px-3').a['href']\n",
    "    info['short_description'] = summary.find('div',  class_='text-xl').text\n",
    "\n",
    "    spans = summary.find_all('span',  class_='ycdc-badge')\n",
    "    info['tags'] = [what.text.replace('Y Combinator Logo', '') for what in spans]\n",
    "    info['description'] = soup.p.text\n",
    "    info['company_socials'] = soup.find('div',  class_='space-x-2')\n",
    "    \n",
    "    spans = []\n",
    "\n",
    "    i = 0\n",
    "    for fact in soup.find('div', class_=\"space-y-0.5\").find_all('span'):\n",
    "        spans.append(fact.text)\n",
    "        try:\n",
    "            key_ = spans[i].lower().replace(' ', '_')[:-1]\n",
    "            info[key_] = spans[i+1]\n",
    "            i += 2\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    info['company_socials'] = [a['href'] for a in soup.find('div',  class_='space-x-2').find_all('a')]\n",
    "\n",
    "    return info\n",
    "\n",
    "\n",
    "def get_founders_info(soup):\n",
    "    founders_info = {}\n",
    "    try:\n",
    "        founders = soup.find('div', class_='space-y-5')\n",
    "        founders_info['active_founders'] = [name.div.text for name in founders.find_all('div', class_='leading-snug')]\n",
    "\n",
    "        about_founder = {}\n",
    "        all_about = []\n",
    "        for what in founders.find_all('div', class_='flex flex-row gap-3 items-start flex-col md:flex-row'):\n",
    "            name = what.h3.text\n",
    "            split = name.split(', ')\n",
    "\n",
    "            about_founder['name'] = name\n",
    "\n",
    "            if split[0] != split[-1]:\n",
    "                about_founder['role'] = split[-1]\n",
    "            else:\n",
    "                about_founder['role'] = ''\n",
    "\n",
    "            # about_founder['summary'] = what.p.text\n",
    "            about_founder['social_media_links'] = [link['href'] for link in what.find('div', class_='mt-1 space-x-2').find_all('a')]\n",
    "            \n",
    "            all_about.append(about_founder)\n",
    "\n",
    "        founders_info['about_founders'] = all_about\n",
    "        \n",
    "    except:\n",
    "        founders = soup.find('div', class_='space-y-4')\n",
    "        founders_info = {}\n",
    "        all_about = []\n",
    "        for founder in founders.find_all('div', class_='leading-snug'):\n",
    "            about_founder = {}\n",
    "            name = founder.find('div', class_='font-bold').text\n",
    "            founders_info['active_founders'] = [name]\n",
    "\n",
    "            about_founder['name'] = name\n",
    "\n",
    "            divs = [ what for what in founder.find_all('div')]\n",
    "            about_founder['role'] = divs[1].text\n",
    "            about_founder['social_media_links'] = [link['href'] for link in founder.find('div', class_='mt-1 space-x-2').find_all('a')]\n",
    "\n",
    "            all_about.append(about_founder)\n",
    "            \n",
    "        founders_info['about_founders'] = all_about\n",
    "\n",
    "    return founders_info\n",
    "\n",
    "\n",
    "def scrape_info(link_href):\n",
    "    main_url = 'https://www.ycombinator.com'\n",
    "    url = main_url + link_href\n",
    "    source = requests.get(url).text\n",
    "    soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "    company_all_info = get_company_info(soup)\n",
    "    try:\n",
    "        founder_info = get_founders_info(soup)\n",
    "    except:\n",
    "        founder_info = {\n",
    "            \"active_founders\":[],\n",
    "            \"about_founders\": []\n",
    "        }\n",
    "    company_all_info.update(founder_info)\n",
    "\n",
    "    return company_all_info\n",
    "\n",
    "\n",
    "def save_to_csv(scraped_info, savepath):\n",
    "       df = pd.DataFrame(scraped_info)\n",
    "       df = df[['company_name', 'link', 'short_description', 'tags',\n",
    "              'company_socials', 'founded', 'team_size', 'location',\n",
    "              'active_founders', 'about_founders', 'description']]\n",
    "              \n",
    "       df.to_csv(savepath, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all links from Y-combinator\n",
    "`run_seleniun_and_get_page_source`: Used to handle the dynamic scraping of the project. It scrolls the website till it reaches the end of the page. Afterward, beautifulsoup is used to extract all the links to the individual company's page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "main_url = 'https://www.ycombinator.com'\n",
    "\n",
    "start = dt.now()\n",
    "get_company_list_block, driver = run_seleniun_and_get_page_source()\n",
    "driver.close()\n",
    "\n",
    "print(\"Selenium closed, handing over to BeautifulSoup\")\n",
    "\n",
    "y_company_page_urls = [link['href'] for link in get_company_list_block]\n",
    "lenght = len(y_company_page_urls)\n",
    "\n",
    "runtime = (dt.now() - start).total_seconds()\n",
    "print(f'Total scroll runtime - {runtime}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information to scrape\n",
    "The image below indicates the imformation to be scraped for analysis\n",
    "\n",
    "\n",
    "<img width=\"1353\" alt=\"Screenshot 2022-04-03 at 7 29 58 PM\" src=\"https://user-images.githubusercontent.com/55639062/161443204-ae7fc423-f1d3-4512-bb56-7bef85f3691e.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping without multi-threading\n",
    "The script ran for `~15 mins` to scrape information from 1000 companies.\n",
    "\n",
    "`Note:` The runtime can be faster or slower depending on internet connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = []\n",
    "retries = []\n",
    "count = 1\n",
    "\n",
    "start_no_thread = dt.now()\n",
    "run = True\n",
    "while run:\n",
    "    for link_href in y_company_page_urls:\n",
    "        try:\n",
    "            print(f\"{count}/{lenght}\", end='\\r')\n",
    "            company_all_info = scrape_info(link_href)\n",
    "            companies.append(company_all_info)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            retries.append(link_href)\n",
    "    \n",
    "    if retries != []:\n",
    "        y_company_page_urls = retries\n",
    "        retries = []\n",
    "    else:\n",
    "        run = False\n",
    "        \n",
    "\n",
    "runtime_no_thread = (dt.now() - start_no_thread).total_seconds()\n",
    "print(f'Total runtime - {runtime_no_thread}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping using multi-threading\n",
    "Multithreading gives a faster and more efficient reuslt, the script ran for about `~62 secs` as compared the the first appoach.\n",
    "\n",
    "`Note:` The runtime can be faster or slower depending on internet connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime - 62.121921\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures as cf\n",
    "\n",
    "\n",
    "start_thread = dt.now()\n",
    "m_companies = []\n",
    "with cf.ThreadPoolExecutor() as exc:\n",
    "    results = exc.map(scrape_info, y_company_page_urls)\n",
    "\n",
    "    for result in results:\n",
    "        m_companies.append(result)\n",
    "\n",
    "runtime_thread = (dt.now() - start_thread).total_seconds()\n",
    "print(f'Total runtime - {runtime_thread}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "save_to_csv(scraped_info=m_companies, savepath='ycombinator.csv')\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ds_env",
   "language": "python",
   "name": ".ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
